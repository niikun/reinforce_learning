{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonコード: 1000状態ランダムウォーク + n-ステップセミ勾配TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 環境設定\n",
    "class RandomWalkEnv:\n",
    "    def __init__(self, num_states=1000, group_size=50):\n",
    "        self.num_states = num_states  # 状態数\n",
    "        self.group_size = group_size  # グループのサイズ（状態集約）\n",
    "        self.terminal_left = 0  # 左端の終端状態\n",
    "        self.terminal_right = num_states + 1  # 右端の終端状態\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.num_states // 2  # 中央から開始\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: -1 (左) または 1 (右)\n",
    "        \"\"\"\n",
    "        if action not in [-1, 1]:\n",
    "            raise ValueError(\"Action must be -1 (left) or 1 (right)\")\n",
    "        \n",
    "        # 次の状態を決定\n",
    "        self.state += action\n",
    "        if self.state == self.terminal_left:\n",
    "            return self.state, -1, True  # 左端に到達 → 報酬 -1\n",
    "        elif self.state == self.terminal_right:\n",
    "            return self.state, 1, True  # 右端に到達 → 報酬 +1\n",
    "        else:\n",
    "            return self.state, 0, False  # 中間状態 → 報酬 0\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return state == self.terminal_left or state == self.terminal_right\n",
    "\n",
    "# 特徴ベクトル作成（状態集約）\n",
    "def create_features(state, num_states, group_size):\n",
    "    features = np.zeros(num_states // group_size)\n",
    "    group = (state - 1) // group_size\n",
    "    if 0 <= group < len(features):\n",
    "        features[group] = 1\n",
    "    return features\n",
    "\n",
    "# n-ステップセミ勾配TDアルゴリズム\n",
    "def n_step_semi_gradient_td_optimized(env, n, alpha, gamma, num_episodes, group_size):\n",
    "    num_features = env.num_states // group_size\n",
    "    w = np.zeros(num_features)  # 重みベクトル（初期化）\n",
    "    \n",
    "    # 特徴ベクトルのキャッシュ\n",
    "    all_features = [create_features(s, env.num_states, group_size) for s in range(env.num_states + 2)]\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # 初期化\n",
    "        state = env.reset()\n",
    "        states = [state]\n",
    "        rewards = []\n",
    "        features = [all_features[state]]\n",
    "\n",
    "        t = 0  # 時刻\n",
    "        T = float('inf')  # 終了時刻\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                # 行動をランダムに選択（左右に等確率で移動）\n",
    "                action = np.random.choice([-1, 1])\n",
    "                next_state, reward, done = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                states.append(next_state)\n",
    "                features.append(all_features[next_state])\n",
    "                \n",
    "                if done:\n",
    "                    T = t + 1\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                # 累積報酬（効率化）\n",
    "                G = sum(gamma ** (i - tau) * rewards[i] for i in range(tau, min(tau + n, T)))\n",
    "                if tau + n < T:\n",
    "                    G += gamma ** n * np.dot(w, features[tau + n])\n",
    "\n",
    "                # 重み更新\n",
    "                w += alpha * (G - np.dot(w, features[tau])) * features[tau]\n",
    "            \n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "            t += 1\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# パラメータ設定\n",
    "num_states = 1000\n",
    "group_size = 50\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "n = 4\n",
    "num_episodes = 100\n",
    "\n",
    "# 環境作成と学習\n",
    "env = RandomWalkEnv(num_states, group_size)\n",
    "w = n_step_semi_gradient_td_optimized(env, n, alpha, gamma, num_episodes, group_size)\n",
    "\n",
    "# 学習結果表示\n",
    "print(\"Learned weights:\", w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
