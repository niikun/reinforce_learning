{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DynaQ-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right\n",
    "height = 6\n",
    "width = 9\n",
    "start = (2, 0)\n",
    "goal = (0, 8)\n",
    "blocks = [(1, 2), (2, 2), (3, 2), (0, 7), (1, 7), (2, 7), (4, 5)]\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 1\n",
    "\n",
    "n_values = [0, 5, 50]\n",
    "\n",
    "def move(state, action):\n",
    "    new_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if new_state[0] < 0 or new_state[0] >= height or new_state[1] < 0 or new_state[1] >= width:\n",
    "        return state\n",
    "    if new_state in blocks:\n",
    "        return state\n",
    "    return new_state\n",
    "\n",
    "def epsilon_greedy(state, q_values):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(len(actions))\n",
    "    return np.argmax(q_values[state[0], state[1]])\n",
    "\n",
    "def dyna_q(n):\n",
    "    history = []    \n",
    "    q_values = np.zeros((height, width, len(actions)))\n",
    "    for i in range(50):  # 50エピソード\n",
    "        state = start\n",
    "        while state != goal:\n",
    "            counter = 0\n",
    "            action = epsilon_greedy(state, q_values)\n",
    "            next_state = move(state, actions[action])\n",
    "            reward = 1 if next_state == goal else 0\n",
    "            q_values[state[0], state[1], action] += alpha * (\n",
    "                reward + gamma * np.max(q_values[next_state[0], next_state[1]]) - q_values[state[0], state[1], action]\n",
    "            )\n",
    "            state = next_state\n",
    "\n",
    "            # 計画ステップ\n",
    "            for _ in range(n):\n",
    "                plan_state = (np.random.randint(height), np.random.randint(width))\n",
    "                if plan_state == goal or plan_state in blocks:\n",
    "                    continue\n",
    "                plan_action = np.random.randint(len(actions))\n",
    "                plan_next_state = move(plan_state, actions[plan_action])\n",
    "                plan_reward = 1 if plan_next_state == goal else 0\n",
    "                q_values[plan_state[0], plan_state[1], plan_action] += alpha * (\n",
    "                    plan_reward + gamma * np.max(q_values[plan_next_state[0], plan_next_state[1]]) - q_values[plan_state[0], plan_state[1], plan_action]\n",
    "                )\n",
    "            counter += 1\n",
    "        history.append(counter)\n",
    "    return q_values, history\n",
    "\n",
    "# 可視化\n",
    "for n in n_values:\n",
    "    q_values,history = dyna_q(n)\n",
    "    plt.imshow(np.max(q_values, axis=2), cmap='hot', interpolation='nearest')\n",
    "    plt.title(f'Q-values (max) after training with n={n}')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.plot(history)\n",
    "    plt.title(f'Number of steps to goal after training with n={n}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
